{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb4454a-2066-4df0-ab45-8edb61783c8b",
   "metadata": {},
   "source": [
    "# What is a Perceptron\n",
    "A perceptron is a building block of neural network. \n",
    "\n",
    "A single perceptron is capable to classify an input into two classes (Binary classifier i-e 0 / 1).\n",
    "![A perceptron looks like this.](./perceptron_node.png \"Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c87fa-45cb-47ee-bf78-b9518ad262b4",
   "metadata": {},
   "source": [
    "## Different parts of a perceptron:\n",
    "\n",
    "1. Input features (X) from dataset.\n",
    "2. Weights (W), one for each input feature and a bias B.\n",
    "3. A Net input Function.\n",
    "4. Activation Function to normalize the values between (0 - 1).\n",
    "5. Output (0/1 or Yes/No or Dog/ Cat etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7e38c-c877-49e6-8174-7bcf043fcacd",
   "metadata": {},
   "source": [
    "## Sonar Dataset\n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a98b402-8f2c-482e-a8cd-0624ab239787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e3c1ed-1008-4a02-b3f8-d7f01d968d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [i for i in range(1,61)]\n",
    "columns.append(\"label\")\n",
    "df = pd.read_csv(\"sonar.all-data\",delimiter = \",\",names = columns,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe21a357-44f5-42fb-b343-60309d150c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1       2       3       4       5       6       7       8       9  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       10  ...      52      53      54      55      56      57      58  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       59      60  label  \n",
       "0  0.0090  0.0032      R  \n",
       "1  0.0052  0.0044      R  \n",
       "2  0.0095  0.0078      R  \n",
       "3  0.0040  0.0117      R  \n",
       "4  0.0107  0.0094      R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d7f74-994a-4f2b-9365-466ca1b6fccc",
   "metadata": {},
   "source": [
    "Replacing R with 0 and M with 1 as our perceptron can only deal with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baba7954-fd00-4828-bc3d-cde76abd9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].replace({'R': 0, 'M': 1},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56116ad1-2b87-4fd0-ba6b-d954a9c60fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[columns[:-1]], df[columns[-1]], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5896d18f-931a-45f8-a8f7-62da3b511850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.2668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.1299</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.0695</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1772</td>\n",
       "      <td>0.1908</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1       2       3       4       5       6       7       8       9   \\\n",
       "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n",
       "42   0.0211  0.0319  0.0415  0.0286  0.0121  0.0438  0.1299  0.1390  0.0695   \n",
       "79   0.0108  0.0086  0.0058  0.0460  0.0752  0.0887  0.1015  0.0494  0.0472   \n",
       "97   0.0491  0.0279  0.0592  0.1270  0.1772  0.1908  0.2217  0.0768  0.1246   \n",
       "142  0.0526  0.0563  0.1219  0.1206  0.0246  0.1022  0.0539  0.0439  0.2291   \n",
       "\n",
       "         10  ...      51      52      53      54      55      56      57  \\\n",
       "28   0.2668  ...  0.0118  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117   \n",
       "42   0.0568  ...  0.0053  0.0090  0.0042  0.0153  0.0106  0.0020  0.0105   \n",
       "79   0.0393  ...  0.0161  0.0029  0.0078  0.0114  0.0083  0.0058  0.0003   \n",
       "97   0.2028  ...  0.0268  0.0081  0.0129  0.0161  0.0063  0.0119  0.0194   \n",
       "142  0.1632  ...  0.0380  0.0339  0.0149  0.0335  0.0376  0.0174  0.0132   \n",
       "\n",
       "         58      59      60  \n",
       "28   0.0020  0.0091  0.0058  \n",
       "42   0.0049  0.0070  0.0080  \n",
       "79   0.0023  0.0026  0.0027  \n",
       "97   0.0140  0.0332  0.0439  \n",
       "142  0.0103  0.0364  0.0208  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04124abc-bbbd-4125-b6c0-f04721bc5d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28     0\n",
       "42     0\n",
       "79     0\n",
       "97     1\n",
       "142    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299e193f-9923-4c43-9669-f5213866481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13de9a2-4990-4ce3-932f-81410f56fca2",
   "metadata": {},
   "source": [
    "## Weights\n",
    "Let's initialize Weights for each input feature. We have 60 features so we need to define 60 wieghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5b3b8e-aa9b-41e4-aa24-880a0736d8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.rand(60,1)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178913b8-4890-4075-afd9-7b3a432bec48",
   "metadata": {},
   "source": [
    "## Bias\n",
    "Let's initialize Bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167e32ae-6e5a-45ca-bd0a-e942ccc19bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6574910695906289"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.rand()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e82abf-4a22-4e6a-a090-a4c796e8e6b0",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "Forward pass contains two steps:\n",
    "1. **Net Input Function:** where we multiply each feature x with it's corresponding weight w and then sum all of the resulting values to get a single value Z. \n",
    "2. **Activation Function:** Applying activaton funciton on Z.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf207ee-ca28-4dab-aaa2-113b0d70ead1",
   "metadata": {},
   "source": [
    "### Net Input Function\n",
    "We have to comput the Net Input Function for all the training samples.\n",
    "\n",
    "\n",
    "![Net input function.](./NIF.png \"Net input function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6b0683-fb48-4141-ade6-1440c8658097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 139)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6ecccb-4654-44f1-8b85-0d90553a1555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, array([0.40494537]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0],W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58fd09a3-d035-436e-971e-1f84f33cee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfTrainSamples = X_train.shape[1]\n",
    "numOfFeatures = X_train.shape[0]\n",
    "Z = np.zeros(numOfTrainSamples)\n",
    "\n",
    "for i in range(numOfTrainSamples):\n",
    "    for j in range(numOfFeatures): \n",
    "        z = float(X_train[j][i] * W[j])\n",
    "        Z[i] = Z[i]+z\n",
    "    Z[i] = Z[i] + b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e866e1-f840-4a19-99af-d530fd86ffb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5fbf00e-0a33-435a-b570-3e1ade618f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.27384711, 9.75198018, 7.7401873 , 8.79860199, 9.64991838])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7468c0d-f8a0-42bb-8528-ab0da92439da",
   "metadata": {},
   "source": [
    "Same net input function can be computed in an optimized manner by using vectorized code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773ec026-8e6a-4405-aee4-9a80d6e7f879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01adc718-e282-4e7f-85d4-5fd9aaed08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 139)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b4633f-91fa-48b4-85c2-2f257ca20283",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.dot(W.T,X_train,) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cd9f08a-a6af-41aa-9bed-06bd4475e83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 139)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a26eb7-123c-4a1d-af04-5bfdc80be4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.27384711, 9.75198018, 7.7401873 , 8.79860199, 9.64991838])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2771418-b3a3-4164-9e76-b1d1858d9e3f",
   "metadata": {},
   "source": [
    "### Activation Funciton\n",
    "We apply activation function to normalize the output values between 0 and 1.\n",
    "Most commonly used Activation Functions are:\n",
    "1. Sigmoid\n",
    "2. Relu\n",
    "3. Leaky Relu\n",
    "4. tanh and more\n",
    "\n",
    "We will use sigmoid for our example.\n",
    "\n",
    "\n",
    "![Sigmoid function.](./sigmoid.png \"Sigmoid Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de62a42a-2f87-41fe-9a70-cfdcdbe72828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "654f5925-cde5-4b29-b6ac-56bccfc21a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [sigmoid(z) for z in Z[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c5564b4-0047-4e4e-ab0e-6459e96745de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9997449630657589,\n",
       " 0.999941824040728,\n",
       " 0.9995651990389985,\n",
       " 0.999849078831124,\n",
       " 0.9999355733251524]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0104085-eb20-4939-a9ee-d3fe23ea2760",
   "metadata": {},
   "source": [
    "More optimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a050f8-395f-46e8-9672-43d6189120f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9cd1c49-c643-4eb9-97ee-eafa1ba2e362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99974496, 0.99994182, 0.9995652 , 0.99984908, 0.99993557])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d8a4f-1a88-449a-a2df-564340f80edc",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "We have computed the output values, now what to do with them? We need the perceptron to answer in Rock / Mine or in other words 0 / 1.\n",
    "We need to apply a threshold on the output values. In most cases a threshold of 0.5 is used. All the output values greater than 0.5 will be considered as 1 and less than 0.5 will be considered as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51f773cd-992b-48ee-ae9d-229858b069bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.where(A < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "411e6627-926e-4997-9aaa-ff1f15a17629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caae408d-8769-43aa-ab86-e1f4a3080d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9885edc1-d53e-4023-ae2d-65ffe2af86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139,)\n",
      "(1, 139)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f7c6825-2fc6-4394-b722-78ebe67b335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train,axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0ace564-e220-4581-8c6c-c7147963958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 139)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd8736-a92b-4ae4-8046-b8f0b12f5bdf",
   "metadata": {},
   "source": [
    "## Output Analysis\n",
    "Our perceptron has not properly categorized the input. We have a lot of errors in it. Let's correct our perceptron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a6c86-e4ca-44f1-86ba-30764e097ea1",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "In Back propagation we compute errors / loss/ cost using a loss function and then tell each weight that how much it has contributed in the error which is done by taking partial derivative of Loss function with respect to each weight. \n",
    "\n",
    "### Error Functions:\n",
    "1. Mean Error Loss\n",
    "2. Mean Squared Error \n",
    "3. Mean Absolute Error\n",
    "4. Mean Squared Logarithmic Error Loss (MSLE)\n",
    "5. Mean Percentage Error\n",
    "6. Mean Absolute Percentage Error\n",
    "7. Binary Classification Losses Binary Cross Entropy\n",
    "8. Multi-Class Cross-Entropy\n",
    "9. Squared Hinge Loss\n",
    "10. Hinge Loss\n",
    "\n",
    "\n",
    "For our example we will be using Binary Cross Entropy Loss:\n",
    "![Binary cross entropy loss.](./loss.png \"Loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf66aa37-453c-4b2c-88ac-0df6397dd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(A, Y):\n",
    "    return -(Y * np.log(A) + (1 - Y) * np.log(1 - A)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dec07dc-997a-4160-864e-434b963e6304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-f812bb7d02b5>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(Y * np.log(A) + (1 - Y) * np.log(1 - A)).mean()\n",
      "<ipython-input-34-f812bb7d02b5>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(Y * np.log(A) + (1 - Y) * np.log(1 - A)).mean()\n"
     ]
    }
   ],
   "source": [
    "J = binary_cross_entropy(A, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb4eec-bc34-45a9-b407-bfcddd8b34a1",
   "metadata": {},
   "source": [
    "Our implementation of loss function cannot handle log of 0 which is equal to 1 ( log(0) = 1 ), that's why we will use library function for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96b131f0-367e-4806-8329-d6feaf872834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "J = log_loss(y_train,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00966af3-7603-41b8-b1ff-25df2b88f9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360.21659711854045"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4bc681-e357-47e1-8c48-4050f17275ac",
   "metadata": {},
   "source": [
    "## Computing Gradients/ Slopes/ Derivatives\n",
    "Below are the partial derivatives of Loss function.\n",
    "\n",
    "![dz.](./dz.png \"dz\")\n",
    "\n",
    "![dw.](./dw.png \"dw\")\n",
    "\n",
    "![db.](./db.png \"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f887cb69-dec0-4a2c-8f13-3cba0d5f19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = A - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "221da50b-94d9-4de7-88d6-ae0a84616fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 139)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d9f7fbf-5e36-4566-ac83-b56525d51972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 139)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67de72f-71b7-4c3f-b5ae-476c5680aa8e",
   "metadata": {},
   "source": [
    "We need to compute derivative of each weight for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4f40575-f3f4-4635-a99c-9b4fb5255f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = np.zeros(len(W))\n",
    "for i in range(len(W)):\n",
    "    for j in range(X_train.shape[1]):\n",
    "        #print(str(i)+ \" \"+ str(j))\n",
    "        #print(X_train[i][j])\n",
    "        dw[i] = dw[i] + dz[0][j]*X_train[i][j]\n",
    "    dw[i] = dw[i]/X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3faad236-f236-485f-9d51-54479642da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0104705 , 0.0154964 , 0.01788489, 0.02003237, 0.03098993])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9aa0a0f-68c3-4eba-bb22-730c75e85663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfTrainSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f75fca-b55b-4e54-a39b-89dfcb400063",
   "metadata": {},
   "source": [
    "More optimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81de314d-e6e3-4cd6-8516-3cedfc106292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw =  np.dot(X_train,dz.T)/numOfTrainSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98807932-c7b3-4301-afe9-d6e785883d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0104705 ],\n",
       "       [0.0154964 ],\n",
       "       [0.01788489],\n",
       "       [0.02003237],\n",
       "       [0.03098993]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e32224-3e72-408f-9b7b-b23ed7b6186a",
   "metadata": {},
   "source": [
    "For bias we need just need the mean of sum of all dz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a1a524d-a517-462e-8dd1-5b03b193d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = np.sum(dz,axis =1)/numOfTrainSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758400c-73b4-4c22-987c-4c4242b13509",
   "metadata": {},
   "source": [
    "## Gradient Desent Step\n",
    "Now we will update all the weights according to their slopes.\n",
    "\n",
    "**Learning Rate (alpha)**\n",
    "alpha is used to control the gradients, if we keep the alpha too high our gradients will diverge from minimum and if we take the alpha too low, the gradients will converge to minimum slowly.\n",
    "\n",
    "alpha range [0,1]\n",
    "\n",
    "let's suppose alpha is 0.001\n",
    "\n",
    "### Update formulas for weight and bias\n",
    "\n",
    "![w_update.](./w_update.png \"w_update\")\n",
    "\n",
    "\n",
    "![b_update.](./b_update.png \"b_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26bdd29a-d413-4d85-b526-f517722eb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57d442b8-1700-4923-9e57-cdfe80c8c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W - alpha * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cdf6512-285e-4bc4-ab03-e75e6b26c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b - alpha *db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878fd66-0493-40c1-9e11-9a04fbf861eb",
   "metadata": {},
   "source": [
    "## Epoch\n",
    "1 Forward and 1 Backward pass is known as 1 epoch.\n",
    "\n",
    "## Task\n",
    "1. Write code to perform N number of epochs until the loss gets close to zero.\n",
    "2. Compute the loss after each epcoh using sklearn loss function.\n",
    "3. Once the perceptron gets trained, test the trained perceptron on testing data and report test accuracy, confusion matrix.\n",
    "4. Try different values of alpha and see how it affects the training process.\n",
    "5. Use the above vectorized code to make 2 layer Neural Network. 1st layer will contain 2 Perceptrons and last layer will contain 1 perceptron. See how it affects the performance using accuracy and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf9193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989b2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01fdbe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 69)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test.T\n",
    "Z = np.dot(W.T,X_test,) + b\n",
    "    #A = [sigmoid(z) for z in Z[0]]\n",
    "A = sigmoid(Z)\n",
    "A = np.where(A < 0.5, 0, 1)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "402bc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.expand_dims(y_train,axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1d41e12-72de-4851-a9bb-f5d1ab1d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5e88d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "360.21659711854045\n",
      "394.2282954113866\n",
      "394.2282954113866\n",
      "392.08108179132716\n",
      "426.07303915000944\n",
      "459.5057337784162\n",
      "459.5057337784162\n",
      "458.94634406798957\n",
      "492.92141109804476\n",
      "560.281237576922\n",
      "559.6995611925322\n",
      "559.1132126586318\n",
      "559.1132126586318\n",
      "591.8641445929369\n",
      "591.2583319874641\n",
      "589.4101320046111\n",
      "622.6902565298027\n",
      "655.2994051587381\n",
      "688.5228306519532\n",
      "687.8561593518343\n",
      "687.8561593518343\n",
      "687.8561593518343\n",
      "788.7431986953383\n",
      "788.7431986953383\n",
      "786.6271085035908\n",
      "786.6271085035908\n",
      "786.6271085035908\n",
      "786.6271085035908\n",
      "786.6271085035908\n",
      "820.4466672581616\n",
      "853.5253949834853\n",
      "887.3230482615228\n",
      "887.3230482615228\n",
      "887.3230482615228\n",
      "886.574323749318\n",
      "885.817840295723\n",
      "919.5922118003211\n",
      "919.5922118003211\n",
      "953.3584942141026\n",
      "953.3584942141026\n",
      "952.5777381085801\n",
      "1020.0682718739438\n",
      "1019.261633832323\n",
      "1052.9847592835706\n",
      "1052.1598917500335\n",
      "1052.1598917500335\n",
      "1052.1598917500335\n",
      "1085.0204333145412\n",
      "1118.7053988067387\n",
      "1118.7053988067387\n",
      "1118.7053988067387\n",
      "1152.3802597934186\n",
      "1152.3802597934186\n",
      "1152.3802597934186\n",
      "1152.3802597934186\n",
      "1152.3802597934186\n",
      "1152.3802597934186\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1151.505997847008\n",
      "1184.264198163855\n",
      "1217.8961305988714\n",
      "1216.9778794977701\n",
      "1216.9778794977701\n",
      "1216.9778794977701\n",
      "1250.5867070109289\n",
      "1250.5867070109289\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1249.6447584579078\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1248.6904965034905\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1247.7235949507394\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1247.7235949507394\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1247.7235949507394\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1247.7235949507394\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1247.7235949507394\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1246.743714466493\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1213.1848185558288\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1212.2049380715825\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1212.2049380715825\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1212.2049380715825\n",
      "1279.2823657456713\n",
      "1212.2049380715825\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1212.2049380715825\n",
      "1212.2049380715825\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1212.2049380715825\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1245.750501866421\n",
      "1246.7437144664932\n",
      "1246.7437144664932\n",
      "1213.1848185558288\n",
      "1212.204938071582\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1178.646042160918\n",
      "1177.6661616766714\n",
      "1177.6661616766714\n",
      "1177.6661616766714\n",
      "1177.6661616766714\n",
      "1177.6661616766714\n",
      "1177.6661616766714\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1176.6729490765997\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1143.1273852817608\n",
      "1109.5684893710966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109.5684893710966\n",
      "1075.9966145289372\n",
      "1176.6729490765997\n",
      "1109.5684893710966\n",
      "1109.5684893710966\n",
      "1142.134172681689\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1108.5886088868501\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1075.0297129761857\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1074.0498324919395\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1107.5953962867784\n",
      "1074.0498324919395\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1039.5110560970288\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "1038.517843496957\n",
      "1072.0497073762072\n",
      "971.4133837914537\n",
      "1072.0497073762072\n",
      "971.4133837914538\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1004.9722797021183\n",
      "1003.9790671020464\n",
      "1037.5109309812965\n",
      "1037.5109309812965\n",
      "1003.9790671020464\n",
      "1037.5109309812965\n",
      "1037.5109309812965\n",
      "1038.517843496957\n",
      "1037.5109309812965\n",
      "1037.5109309812965\n",
      "1038.517843496957\n",
      "1037.5109309812965\n",
      "1003.9790671020464\n",
      "1003.9790671020464\n",
      "1003.9790671020464\n",
      "1002.9721545863857\n",
      "1002.9721545863857\n",
      "1002.9721545863857\n",
      "1003.9790671020464\n",
      "1002.9721545863857\n",
      "1002.9721545863857\n",
      "1003.9790671020464\n",
      "1002.9721545863857\n",
      "1003.9790671020464\n",
      "1002.9721545863857\n",
      "1003.9790671020464\n",
      "1002.9721545863857\n",
      "1003.9790671020464\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "935.8947269122968\n",
      "969.4402907071357\n",
      "935.8947269122968\n",
      "935.8947269122968\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "968.4333781914751\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "971.4133837914537\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "969.4402907071357\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "970.4335033072075\n",
      "936.8746073965431\n",
      "969.4402907071357\n",
      "936.8746073965431\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "936.8746073965431\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "936.8746073965431\n",
      "934.901514312225\n",
      "936.8746073965431\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "936.8746073965431\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "936.8746073965431\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "936.8746073965431\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "970.4335033072076\n",
      "968.4333781914751\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "970.4335033072076\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "969.4402907071357\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "901.3559505173862\n",
      "900.3627379173143\n",
      "901.3559505173862\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "901.3559505173862\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "900.3627379173143\n",
      "900.3627379173143\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "902.3358310016324\n",
      "967.4123825273191\n",
      "902.3358310016324\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "934.901514312225\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "935.8947269122968\n",
      "935.8947269122968\n",
      "968.4333781914751\n",
      "902.3358310016324\n",
      "968.4333781914751\n",
      "902.3358310016324\n",
      "967.4123825273191\n",
      "902.3358310016324\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "934.901514312225\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "901.3559505173862\n",
      "933.8946017965644\n",
      "901.3559505173862\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "900.3627379173142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901.3559505173862\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "900.3627379173142\n",
      "900.3627379173142\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "for i in range(1000):\n",
    "    #forward\n",
    "    Z = np.dot(W.T,X_train,) + b\n",
    "    A = sigmoid(Z)\n",
    "    A = np.where(A < 0.5, 0 ,1)\n",
    "\n",
    "\n",
    "    #backward\n",
    "    J = log_loss(y_train, A)\n",
    "    print(J)\n",
    "    \n",
    "    dz = A - y_train\n",
    "    dw = np.dot(X_train,dz.T)/numOfTrainSamples\n",
    "    db = np.sum(dz,axis=1)/numOfTrainSamples\n",
    "    W = W - alpha * dw\n",
    "    b = b - alpha * db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d2f2d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900.3627379173142\n"
     ]
    }
   ],
   "source": [
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a8245b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553956834532374"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train[0], A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ed83df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train.argmax(axis=1), A.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d452ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a69bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490e704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
